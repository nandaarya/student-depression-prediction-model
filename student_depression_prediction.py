# -*- coding: utf-8 -*-
"""student-depression-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1biV7ClBS6s9qltq-xJ5c7qZ_Wf_vUNGf
"""

!pip install jupyter scikit-learn tensorflow tfx flask joblib tensorflow_model_analysis==0.46.0

import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os

PIPELINE_NAME = "nandaaryaputra-pipeline"
SCHEMA_PIPELINE_NAME = "student-depression-prediction-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

# from absl import logging
# logging.set_verbosity(logging.INFO)

DATA_ROOT = "/kaggle/input/student-depression-dataset/"

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""# Data Ingestion

## Load Dataset Using ExampleGen
"""

output_config = example_gen_pb2.Output(
    split_config=example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2),
    ])
)

# Load dataset from folder "data"
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output_config)

# run pipeline
interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)
interactive_context.run(example_gen)

"""# Data Validation

## Create summary statistic using StatisticsGen
"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)


interactive_context.run(statistics_gen)

interactive_context.show(statistics_gen.outputs["statistics"])

"""## Create data schema using SchemaGen"""

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"])
interactive_context.run(schema_gen)

interactive_context.show(schema_gen.outputs["schema"])

"""## Identifying anomalies in dataset using ExampleValidator"""

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

interactive_context.show(example_validator.outputs['anomalies'])

"""# Data Preprocessing

## Create Preprocessing Function
"""

TRANSFORM_MODULE_FILE = "dataset_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# 
# LABEL_KEY = "Depression"
# NUMERIC_FEATURES = ["Academic Pressure", "Age", "CGPA", "Study Satisfaction", "Work/Study Hours"]
# CATEGORICAL_FEATURES = ["Dietary Habits", "Family History of Mental Illness",
#                         "Financial Stress", "Gender", "Have you ever had suicidal thoughts ?",
#                         "Sleep Duration"]
# 
# UNUSED_FEATURES = ["Job Satisfaction", "Work Pressure", "id", "City", "Profession", "Degree"]
# 
# def transformed_name(key):
#     """Renames transformed features."""
#     return key.replace("/", "_").replace(" ", "_").lower() + "_xf"
# 
# def preprocessing_fn(inputs):
#     """
#     Applies preprocessing to input features.
# 
#     Args:
#         inputs: A dictionary mapping feature keys to raw tensors.
# 
#     Returns:
#         outputs: A dictionary mapping feature keys to transformed tensors.
#     """
#     outputs = {}
# 
#     # Remove unused features
#     filtered_inputs = {key: tensor for key, tensor in inputs.items() if key not in UNUSED_FEATURES}
# 
#     # Remove data where 'Financial Stress' is missing ("?")
#     mask = tf.not_equal(filtered_inputs["Financial Stress"], "?")
#     clean_inputs = {key: tf.boolean_mask(tensor, mask) for key, tensor in filtered_inputs.items()}
# 
#     # Standardize numerical features using Z-score normalization
#     for feature in NUMERIC_FEATURES:
#         outputs[transformed_name(feature)] = tft.scale_to_z_score(clean_inputs[feature])
# 
#     # One-Hot Encoding for categorical features
#     for feature in CATEGORICAL_FEATURES:
#         vocab_filename = feature.replace(" ", "_").lower() + "_vocab"
# 
#         # Apply encoding using the created vocabulary
#         indexed = tft.compute_and_apply_vocabulary(
#             clean_inputs[feature],
#             vocab_filename=vocab_filename,
#             num_oov_buckets=1
#         )
# 
#         vocab_size = tf.cast(tft.experimental.get_vocabulary_size_by_name(vocab_filename) + 1, tf.int32)
# 
#         # One-Hot Encoding
#         one_hot_encoded = tf.one_hot(indexed, depth=vocab_size, on_value=1.0, off_value=0.0)
# 
#         outputs[transformed_name(feature)] = one_hot_encoded
# 
#     # Retain the target label without modification
#     outputs[LABEL_KEY] = clean_inputs[LABEL_KEY]
# 
#     return outputs

"""## Transform data using Transform Component and Preprocessing Function"""

transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)
interactive_context.run(transform)

"""# Model Training

## Tuner Hyperparameter
"""

TUNER_MODULE_FILE = "model_tuner.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TUNER_MODULE_FILE}
# 
# import keras_tuner as kt
# import tensorflow as tf
# import tensorflow_transform as tft
# from typing import NamedTuple, Dict, Text, Any
# from keras_tuner.engine import base_tuner
# from tensorflow.keras import layers
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = "Depression"
# NUMERIC_FEATURES = ["Academic Pressure", "Age", "CGPA", "Study Satisfaction", "Work/Study Hours"]
# CATEGORICAL_FEATURES = ["Dietary Habits", "Family History of Mental Illness",
#                         "Financial Stress", "Gender", "Have you ever had suicidal thoughts ?", "Sleep Duration"]
# NUM_EPOCHS = 10
# 
# TunerFnResult = NamedTuple("TunerFnResult", [
#     ("tuner", base_tuner.BaseTuner),
#     ("fit_kwargs", Dict[Text, Any]),
# ])
# 
# def transformed_name(key):
#     return key.replace("/", "_").replace(" ", "_").lower() + "_xf"
# 
# def gzip_reader_fn(filenames):
#     return tf.data.TFRecordDataset(filenames, compression_type="GZIP")
# 
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     feature_spec = tf_transform_output.transformed_feature_spec().copy()
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=LABEL_KEY)
#     return dataset
# 
# def model_builder(hp, tf_transform_output):
#     inputs = {}
#     numeric_inputs = [tf.keras.Input(shape=(1,), name=transformed_name(f), dtype=tf.float32) for f in NUMERIC_FEATURES]
#     for f in NUMERIC_FEATURES:
#         inputs[transformed_name(f)] = numeric_inputs[NUMERIC_FEATURES.index(f)]
# 
#     concat_numeric = layers.concatenate(numeric_inputs)
#     x = layers.Dense(hp.Int("dense_units_1", min_value=32, max_value=128, step=32), activation='relu')(concat_numeric)
# 
#     categorical_inputs = []
#     for feature in CATEGORICAL_FEATURES:
#         transformed_feature_name = transformed_name(feature)
#         vocab_size = tf_transform_output.vocabulary_size_by_name(feature.replace(" ", "_").lower() + "_vocab") + 1
# 
#         categorical_input = tf.keras.Input(shape=(vocab_size,), name=transformed_feature_name, dtype=tf.float32)
#         categorical_inputs.append(categorical_input)
#         inputs[transformed_feature_name] = categorical_input
# 
#     concat_categorical = layers.concatenate(categorical_inputs)
#     combined = layers.concatenate([x, concat_categorical])
#     x = layers.Dense(hp.Int("dense_units_2", min_value=32, max_value=128, step=32), activation='relu')(combined)
#     x = layers.Dropout(hp.Float("dropout_rate", min_value=0.1, max_value=0.5, step=0.1))(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
#     model.compile(
#         loss='binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(hp.Choice("learning_rate", values=[1e-2, 1e-3, 1e-4])),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
#     )
#     return model
# 
# def tuner_fn(fn_args: FnArgs):
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
#     train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS)
#     eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS)
# 
#     tuner = kt.BayesianOptimization(
#         hypermodel=lambda hp: model_builder(hp, tf_transform_output),
#         objective=kt.Objective('binary_accuracy', direction='max'),
#         max_trials=30,
#         directory=fn_args.working_dir,
#         project_name="bayesian_tuning",
#     )
# 
#     return TunerFnResult(
#         tuner=tuner,
#         fit_kwargs={
#             "x": train_set,
#             "validation_data": eval_set,
#             "epochs": NUM_EPOCHS,
#         },
#     )

from tfx.proto import trainer_pb2

tuner = Tuner(
    module_file=os.path.abspath(TUNER_MODULE_FILE),
    examples=transform.outputs["transformed_examples"],
    transform_graph=transform.outputs["transform_graph"],
    schema=schema_gen.outputs["schema"],
    train_args=trainer_pb2.TrainArgs(splits=["train"], num_steps=800),
    eval_args=trainer_pb2.EvalArgs(splits=["eval"], num_steps=400),
)
interactive_context.run(tuner)

"""## Trainer"""

TRAINER_MODULE_FILE = "model_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# 
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = "Depression"
# NUMERIC_FEATURES = ["Academic Pressure", "Age", "CGPA", "Study Satisfaction", "Work/Study Hours"]
# CATEGORICAL_FEATURES = ["Dietary Habits", "Family History of Mental Illness",
#                         "Financial Stress", "Gender", "Have you ever had suicidal thoughts ?", "Sleep Duration"]
# 
# def transformed_name(key):
#     """Convert feature name to match transformed feature in preprocessing."""
#     return key.replace("/", "_").replace(" ", "_").lower() + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Read compressed TFRecord dataset."""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     """Load and parse transformed dataset."""
#     feature_spec = tf_transform_output.transformed_feature_spec().copy()
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=LABEL_KEY)
#     return dataset
# 
# def model_builder(tf_transform_output):
#     """Build the deep learning model using transformed features."""
#     inputs = {}
# 
#     # Numeric Features (already normalized)
#     numeric_inputs = [tf.keras.Input(shape=(1,), name=transformed_name(f), dtype=tf.float32) for f in NUMERIC_FEATURES]
#     for f in NUMERIC_FEATURES:
#         inputs[transformed_name(f)] = numeric_inputs[NUMERIC_FEATURES.index(f)]
# 
#     # Concatenate numeric inputs
#     concat_numeric = layers.concatenate(numeric_inputs)
#     x = layers.Dense(64, activation='relu')(concat_numeric)
#     x = layers.Dense(32, activation='relu')(x)
# 
#     # Categorical Features (One-Hot Encoded)
#     categorical_inputs = []
#     for feature in CATEGORICAL_FEATURES:
#         transformed_feature_name = transformed_name(feature)
#         vocab_size = tf_transform_output.vocabulary_size_by_name(feature.replace(" ", "_").lower() + "_vocab") + 1
# 
#         categorical_input = tf.keras.Input(shape=(vocab_size,), name=transformed_feature_name, dtype=tf.float32)
#         categorical_inputs.append(categorical_input)
#         inputs[transformed_feature_name] = categorical_input
# 
#     # Concatenate categorical inputs
#     concat_categorical = layers.concatenate(categorical_inputs)
# 
#     # Combine Numeric & Categorical Features
#     combined = layers.concatenate([x, concat_categorical])
#     x = layers.Dense(64, activation='relu')(combined)
#     x = layers.Dense(32, activation='relu')(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
#     model.compile(
#         loss='binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
#     )
#     model.summary()
#     return model
# 
# def _get_serve_tf_example_fn(model, tf_transform_output):
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
#         feature_spec = tf_transform_output.raw_feature_spec()
#         feature_spec.pop(LABEL_KEY)
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
#         transformed_features = model.tft_layer(parsed_features)
# 
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs):
#     """Train and save the model."""
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(
#         filepath=os.path.join(fn_args.serving_model_dir, "best_model.keras"),
#         monitor='val_binary_accuracy',
#         mode='max',
#         save_best_only=True
#     )
# 
#     # Load TFT transform graph
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Load training and validation datasets
#     train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)
# 
#     # Build and train the model
#     model = model_builder(tf_transform_output)
#     model.fit(
#         x=train_set,
#         validation_data=val_set,
#         callbacks=[tensorboard_callback, es, mc],
#         epochs=10)
# 
#     signatures = {
#     "serving_default": _get_serve_tf_example_fn(
#         model, tf_transform_output
#     ).get_concrete_function(
#         tf.TensorSpec(
#             shape=[None],
#             dtype=tf.string,
#             name="examples"
#         )
#     )}
# 
#     # Save final model
#     tf.saved_model.save(model, fn_args.serving_model_dir, signatures=signatures)

from tfx.proto import trainer_pb2

trainer = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples=transform.outputs["transformed_examples"],
    transform_graph=transform.outputs["transform_graph"],
    schema=schema_gen.outputs["schema"],
    hyperparameters=tuner.outputs["best_hyperparameters"],
    train_args=trainer_pb2.TrainArgs(splits=["train"], num_steps=800),
    eval_args=trainer_pb2.EvalArgs(splits=["eval"], num_steps=400),
)

interactive_context.run(trainer)

"""# Model Analysis and Validation

## Get model using Resolver Component
"""

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

"""## Evaluate model using Evaluator Component"""

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='Depression')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""## Export Model Using Pusher Component"""

from tfx.components import Pusher
from tfx.proto import pusher_pb2

pusher = Pusher(
    model=trainer.outputs["model"],
    model_blessing=evaluator.outputs["blessing"],
    push_destination=pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory=os.path.join(
                SERVING_MODEL_DIR, "student-depression-prediction-model"
            ),
        )
    )
)

interactive_context.run(pusher)

"""# Create Requirement.txt"""

!pip freeze >> requirements.txt